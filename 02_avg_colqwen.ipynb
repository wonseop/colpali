{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Search Optimization with Average Vector + Rescoring\n",
    "\n",
    "This notebook demonstrates the **Average Vector and Rescoring** techniques from Elastic's Search Labs blog post, '[Scaling late interaction models in Elasticsearch](https://www.elastic.co/search-labs/blog/scale-late-interaction-model-colpali)'.\n",
    "\n",
    "**Demo Scenario:**\n",
    "1.  **Problem Definition**: For large-scale search, we create a single average vector per document to leverage Elasticsearch's fast `knn` search.\n",
    "2.  **Identifying the Trade-off**: We observe a drop in search quality (ranking degradation) when using `knn` search alone, compared to the high-quality results from Part 1.\n",
    "3.  **Presenting the Solution**: We use Elasticsearch's `rescore` API to perform a precise, secondary scoring calculation on the initial `knn` results using `rank_vectors`.\n",
    "4.  **Verifying the Result**: We show that rescoring successfully restores the top-quality document to the #1 rank, proving the effectiveness of this two-stage architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Environment Setup and Library Installation\n",
    "\n",
    "**[EN]** Install the necessary libraries, same as in Part 1.<br>\n",
    "**[KR]** Part 1과 동일하게 필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"git+https://github.com/illuin-tech/colpali.git\"\n",
    "!pip install -q \"transformers>=4.41.0\" accelerate Pillow elasticsearch python-dotenv tqdm numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Set Environment Variables\n",
    "\n",
    "**[EN]** Use the same environment setup as in Part 1. Define a new index name and field names for this optimization demo.<br>\n",
    "**[KR]** Part 1과 동일한 환경 설정을 사용합니다. 이번 최적화 데모를 위해 새로운 인덱스 이름과 필드 이름을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dotenv_path = 'elastic.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "ELASTIC_HOST = os.getenv(\"ELASTIC_HOST\")\n",
    "ELASTIC_API_KEY = os.getenv(\"ELASTIC_API_KEY\")\n",
    "\n",
    "if not ELASTIC_HOST or not ELASTIC_API_KEY:\n",
    "    raise ValueError(f\"Please create an '{dotenv_path}' file and set ELASTIC_HOST and ELASTIC_API_KEY variables.\")\n",
    "\n",
    "INDEX_NAME = \"colqwen-rvlcdip-demo-part2\"\n",
    "VECTOR_FIELD_NAME = \"colqwen_vectors\"\n",
    "AVG_VECTOR_FIELD_NAME = \"colqwen_avg_vector\"\n",
    "SAMPLED_DATA_DIR = \"samples/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load ColQwen Model and Verify Data Path\n",
    "\n",
    "**[EN]** Load the same model as in Part 1 and verify the path to the RVL-CDIP dataset.<br>\n",
    "**[KR]** Part 1과 동일한 모델을 로드하고, RVL-CDIP 데이터셋 경로를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55e8e3061b46568460954c86fc1c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097bc96caf6c41478f97e4d1a2f82be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'tsystems/colqwen2.5-3b-multilingual-v1.0' loaded successfully.\n",
      "Found 1600 sample images.\n"
     ]
    }
   ],
   "source": [
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "device_map = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device_map = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_map = \"cuda:0\"\n",
    "print(f\"Using device: {device_map}\")\n",
    "\n",
    "MODEL_NAME = \"tsystems/colqwen2.5-3b-multilingual-v1.0\"\n",
    "model = ColQwen2_5.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if device_map != \"cpu\" else torch.float32,\n",
    "    device_map=device_map\n",
    ").eval()\n",
    "\n",
    "processor = ColQwen2_5_Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Model '{MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "image_paths = []\n",
    "if os.path.exists(SAMPLED_DATA_DIR):\n",
    "    for category_dir in os.listdir(SAMPLED_DATA_DIR):\n",
    "        full_category_path = os.path.join(SAMPLED_DATA_DIR, category_dir)\n",
    "        if os.path.isdir(full_category_path):\n",
    "            image_paths.extend(glob.glob(os.path.join(full_category_path, '*.*')))\n",
    "print(f\"Found {len(image_paths)} sample images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Helper Functions for Embeddings\n",
    "\n",
    "**[EN]** In addition to the existing embedding functions, define a new function to calculate a single, normalized average vector from a set of multi-vectors.<br>\n",
    "**[KR]** 기존 임베딩 생성 함수와 더불어, 다중 벡터를 입력받아 정규화된 단일 평균 벡터를 계산하는 함수를 추가로 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colqwen_document_vectors(image_path):\n",
    "    \"\"\"Generates multi-vector embeddings for a document image.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor.process_images([image]).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_colqwen_query_vectors(query_text):\n",
    "    \"\"\"Generates multi-vector embeddings for a text query.\"\"\"\n",
    "    inputs = processor.process_queries([query_text]).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "\n",
    "def calculate_average_vector(vectors):\n",
    "    \"\"\"Calculates a single, normalized average vector from multi-vectors.\"\"\"\n",
    "    if not vectors or len(vectors) == 0:\n",
    "        return None\n",
    "    avg_vec = np.array(vectors).mean(axis=0)\n",
    "    norm = np.linalg.norm(avg_vec)\n",
    "    if norm == 0:\n",
    "        return avg_vec.tolist()\n",
    "    return (avg_vec / norm).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Elasticsearch Index with Two Vector Fields\n",
    "\n",
    "**[EN]** This is the core of the demo. We create an index with both a `dense_vector` field for the average vector (for `knn` search) and a `rank_vectors` field for the original multi-vectors (for `rescoring`).<br>\n",
    "**[KR]** 이 데모의 핵심입니다. 하나의 인덱스에 `knn` 검색을 위한 `dense_vector` 타입의 평균 벡터 필드와 `rescoring`을 위한 `rank_vectors` 타입의 원본 다중 벡터 필드를 모두 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch version: 9.0.1\n",
      "Deleted existing index: colqwen-rvlcdip-demo-part2\n",
      "Created index 'colqwen-rvlcdip-demo-part2'.\n"
     ]
    }
   ],
   "source": [
    "if \":\" in ELASTIC_HOST and not ELASTIC_HOST.startswith(\"http\"):\n",
    "  es = Elasticsearch(cloud_id=ELASTIC_HOST, api_key=ELASTIC_API_KEY)\n",
    "else:\n",
    "  es = Elasticsearch(hosts=[ELASTIC_HOST], api_key=ELASTIC_API_KEY)\n",
    "\n",
    "print(f\"Connected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"Deleted existing index: {INDEX_NAME}\")\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        AVG_VECTOR_FIELD_NAME: { 'type': 'dense_vector', 'dims': 128, 'index': True, 'similarity': 'dot_product' },\n",
    "        VECTOR_FIELD_NAME: { 'type': 'rank_vectors', 'dims': 128 },\n",
    "        \"image_path\": {\"type\": \"keyword\"},\n",
    "        \"category\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=INDEX_NAME, mappings=mapping)\n",
    "print(f\"Created index '{INDEX_NAME}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Index Data with Both Vector Types\n",
    "\n",
    "**[EN]** For each document, calculate both the original multi-vectors and the average vector, then index them together in a single document.<br>\n",
    "**[KR]** 각 문서에 대해 원본 다중 벡터와 평균 벡터를 모두 계산하여 하나의 문서에 함께 인덱싱합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 1600 documents... This may take a while.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa77e45b9b341b29430764a5ec534f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Indexing Documents:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m doc_id = os.path.splitext(os.path.basename(path))[\u001b[32m0\u001b[39m]\n\u001b[32m      7\u001b[39m category = os.path.basename(os.path.dirname(path))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m multi_vectors = \u001b[43mcreate_colqwen_document_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multi_vectors:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcreate_colqwen_document_vectors\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m      5\u001b[39m     inputs = processor.process_images([image]).to(model.device)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs.cpu().to(torch.float32).numpy().tolist()[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py:97\u001b[39m, in \u001b[36mColQwen2_5.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m] = torch.cat(\n\u001b[32m     87\u001b[39m         [pixel_sequence[:offset] \u001b[38;5;28;01mfor\u001b[39;00m pixel_sequence, offset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mpixel_values\u001b[39m\u001b[33m\"\u001b[39m], offsets)],\n\u001b[32m     88\u001b[39m         dim=\u001b[32m0\u001b[39m,\n\u001b[32m     89\u001b[39m     )\n\u001b[32m     91\u001b[39m position_ids, rope_deltas = \u001b[38;5;28mself\u001b[39m.get_rope_index(\n\u001b[32m     92\u001b[39m     input_ids=kwargs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     93\u001b[39m     image_grid_thw=kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mimage_grid_thw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m     94\u001b[39m     video_grid_thw=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     95\u001b[39m     attention_mask=kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m     96\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m last_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, sequence_length, hidden_size)\u001b[39;00m\n\u001b[32m    105\u001b[39m proj = \u001b[38;5;28mself\u001b[39m.custom_text_proj(last_hidden_states)  \u001b[38;5;66;03m# (batch_size, sequence_length, dim)\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# L2 normalization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/colpali_engine/models/qwen2_5/colqwen2_5/modeling_colqwen2_5.py:50\u001b[39m, in \u001b[36mColQwen2_5.inner_forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     49\u001b[39m     pixel_values = pixel_values.type(\u001b[38;5;28mself\u001b[39m.visual.dtype)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     image_mask = (input_ids == \u001b[38;5;28mself\u001b[39m.config.image_token_id).unsqueeze(-\u001b[32m1\u001b[39m).expand_as(inputs_embeds)\n\u001b[32m     52\u001b[39m     image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:546\u001b[39m, in \u001b[36mQwen2_5_VisionTransformerPretrainedModel.forward\u001b[39m\u001b[34m(self, hidden_states, grid_thw)\u001b[39m\n\u001b[32m    542\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    543\u001b[39m             blk.\u001b[34m__call__\u001b[39m, hidden_states, cu_seqlens_now, \u001b[38;5;28;01mNone\u001b[39;00m, position_embeddings\n\u001b[32m    544\u001b[39m         )\n\u001b[32m    545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m         hidden_states = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens_now\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    548\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.merger(hidden_states)\n\u001b[32m    549\u001b[39m reverse_indices = torch.argsort(window_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:339\u001b[39m, in \u001b[36mQwen2_5_VLVisionBlock.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    333\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    334\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m     position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    338\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(hidden_states))\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/pytorch/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py:302\u001b[39m, in \u001b[36mQwen2_5_VLVisionSdpaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings)\u001b[39m\n\u001b[32m    300\u001b[39m attention_mask = torch.zeros([\u001b[32m1\u001b[39m, seq_length, seq_length], device=q.device, dtype=torch.bool)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cu_seqlens)):\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     attention_mask[..., cu_seqlens[i - \u001b[32m1\u001b[39m] : cu_seqlens[i], cu_seqlens[i - \u001b[32m1\u001b[39m] : cu_seqlens[i]] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    303\u001b[39m q = q.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    304\u001b[39m k = k.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MAX_DOCS_TO_INDEX = 1600\n",
    "docs_to_index = image_paths[:min(len(image_paths), MAX_DOCS_TO_INDEX)]\n",
    "\n",
    "print(f\"Indexing {len(docs_to_index)} documents... This may take a while.\")\n",
    "for path in tqdm(docs_to_index, desc=\"Indexing Documents\"):\n",
    "    doc_id = os.path.splitext(os.path.basename(path))[0]\n",
    "    category = os.path.basename(os.path.dirname(path))\n",
    "    \n",
    "    multi_vectors = create_colqwen_document_vectors(path)\n",
    "    if not multi_vectors:\n",
    "        continue\n",
    "    \n",
    "    avg_vector = calculate_average_vector(multi_vectors)\n",
    "    \n",
    "    es_doc = {\n",
    "        VECTOR_FIELD_NAME: multi_vectors,\n",
    "        AVG_VECTOR_FIELD_NAME: avg_vector,\n",
    "        \"image_path\": path,\n",
    "        \"category\": category\n",
    "    }\n",
    "    es.index(index=INDEX_NAME, id=doc_id, document=es_doc)\n",
    "\n",
    "es.indices.refresh(index=INDEX_NAME)\n",
    "print(f\"\\nIndexing complete. Total documents: {es.count(index=INDEX_NAME)['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Demonstrate the Problem: KNN Search Only\n",
    "\n",
    "**[EN]** First, let's perform a search using only the fast `knn` query. Pay attention to how the rank of the document that was #1 in Part 1 changes. This demonstrates the trade-off: we gain speed but lose some precision.<br>\n",
    "**[KR]** 먼저 빠른 `knn` 검색만 사용하여 결과를 확인합니다. Part 1에서 1위를 차지했던 문서의 순위가 어떻게 변하는지 주목합니다. 이는 속도를 얻는 대신 정밀도를 일부 잃는 트레이드오프를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_to_test = [\n",
    "    \"Find the return request from the customer\",\n",
    "    \"고객사에서 보낸 반품요청서를 찾아줘\",\n",
    "    \"I need to find an internal memo that shows how a customer complaint was handled.\",\n",
    "    \"고객 불만 처리 내역이 적힌 내부 메모를 찾아야 해\",\n",
    "    \"Look for flyers or advertising materials for summer sales.\",\n",
    "    \"여름 세일 안내 전단지나 광고 자료를 찾아줘.\",\n",
    "    \"Show me the receipt you received after purchasing a product that shows the tax line item\",\n",
    "    \"제품 구매 후 받은 영수증 중 세금 항목이 적힌 걸 보여줘\",\n",
    "    \"Do you have a benefits policy change notice from HR?\",\n",
    "    \"인사팀에서 보내온 복리후생 정책 변경 안내문이 있나?\",\n",
    "    \"Do you have any faxes from last July with handwritten notes?\",\n",
    "    \"지난 7월에 보낸 팩스 중에서 필기체로 메모된 게 있어?\",\n",
    "    \"Where's the company's marketing strategy announcement?\",\n",
    "    \"회사의 마케팅 전략 발표 자료는 어디 있어?\"\n",
    "]\n",
    "\n",
    "def display_results(hits):\n",
    "    html = \"<table><tr>\"\n",
    "    for i, hit in enumerate(hits):\n",
    "        doc_id = hit[\"_id\"]\n",
    "        score = hit[\"_score\"]\n",
    "        path = hit[\"_source\"][\"image_path\"]\n",
    "        category = hit[\"_source\"][\"category\"]\n",
    "        try:\n",
    "            with open(path, \"rb\") as image_file:\n",
    "                img_str = base64.b64encode(image_file.read()).decode()\n",
    "                html += f\"<td style='text-align: center; vertical-align: top; padding: 10px; border: 1px solid #ddd;'>\"\n",
    "                html += f\"<b>Rank #{i+1}</b><br><img src='data:image/png;base64,{img_str}' width='200'><br>\"\n",
    "                html += f\"<b>ID:</b> {doc_id[:15]}...<br><b>Score:</b> {score:.4f}<br><b>Category:</b> {category}</td>\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image {path}: {e}\")\n",
    "    html += \"</tr></table>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "for query in queries_to_test:\n",
    "    print(f\"\\n{'='*20}\\n--- Scenario: KNN Search Only ---\")\n",
    "    print(f\"Searching for: '{query}'\\n{'='*20}\")\n",
    "\n",
    "    query_avg_vector = calculate_average_vector(create_colqwen_query_vectors(query))\n",
    "    \n",
    "    knn_query = {\n",
    "        \"field\": AVG_VECTOR_FIELD_NAME,\n",
    "        \"query_vector\": query_avg_vector,\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "    \n",
    "    results = es.search(index=INDEX_NAME, knn=knn_query, size=5, source=[\"image_path\", \"category\"])\n",
    "    display_results(results['hits']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Present the Solution: KNN + Rescoring\n",
    "\n",
    "**[EN]** Now, we solve this problem using the `rescore` API. For the top 10 documents found by the initial `knn` search, we recalculate a more precise score using `rank_vectors` to determine the final ranking. Observe how the original top-ranked document is restored.<br>\n",
    "**[KR]** 이제 `rescore` API를 사용하여 이 문제를 해결합니다. 1차 `knn` 검색으로 찾은 상위 10개 문서에 대해서만, `rank_vectors`를 사용한 정밀한 점수 계산을 다시 수행하여 최종 순위를 결정합니다. 원래 1위였던 문서의 순위가 어떻게 복원되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in queries_to_test:\n",
    "    print(f\"\\n{'='*20}\\n--- Scenario: KNN with Rescoring ---\")\n",
    "    print(f\"Searching for: '{query}'\\n{'='*20}\")\n",
    "\n",
    "    query_multi_vectors = create_colqwen_query_vectors(query)\n",
    "    query_avg_vector = calculate_average_vector(query_multi_vectors)\n",
    "    \n",
    "    knn_query = {\n",
    "        \"field\": AVG_VECTOR_FIELD_NAME,\n",
    "        \"query_vector\": query_avg_vector,\n",
    "        \"k\": 10,\n",
    "        \"num_candidates\": 100\n",
    "    }\n",
    "    \n",
    "    rescore_definition = {\n",
    "        \"window_size\": 10,\n",
    "        \"query\": {\n",
    "            \"rescore_query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\"match_all\": {}},\n",
    "                    \"script\": {\n",
    "                        \"source\": f\"maxSimDotProduct(params.query_vector, '{VECTOR_FIELD_NAME}')\",\n",
    "                        \"params\": {\"query_vector\": query_multi_vectors}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"query_weight\": 0.0,\n",
    "            \"rescore_query_weight\": 1.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = es.search(\n",
    "        index=INDEX_NAME, \n",
    "        knn=knn_query, \n",
    "        rescore=rescore_definition, \n",
    "        size=5, \n",
    "        source=[\"image_path\", \"category\"]\n",
    "    )\n",
    "    display_results(results['hits']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Free up GPU/MPS Memory\n",
    "\n",
    "**[EN]** As a best practice, explicitly delete the model and processor to free up GPU or system memory after the demonstration is complete.<br>\n",
    "**[KR]** 모범 사례로서, 데모가 완료된 후 모델과 프로세서를 명시적으로 삭제하여 GPU 또는 시스템 메모리를 확보합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del processor\n",
    "\n",
    "if 'torch' in locals() and 'mps' in locals() and device_map == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared.\")\n",
    "elif 'torch' in locals() and 'cuda' in locals() and device_map == \"cuda:0\":\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n",
    "else:\n",
    "    print(\"CPU memory will be freed by Python's garbage collector.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
