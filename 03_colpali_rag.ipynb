{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Interactive Multimodal RAG with boto3\n",
    "\n",
    "**[EN]** This notebook implements a complete, interactive multimodal RAG demo. It uses Colpali for visual search and directly calls Amazon Bedrock's Claude 3 Haiku model via boto3 to analyze the top-ranked image and generate a response. All code comments are in English.\n",
    "**[KR]** 이 Notebook은 완전한 대화형 멀티모달 RAG 데모를 구현합니다. Colpali를 사용하여 시각적 검색을 수행하고, boto3를 통해 Amazon Bedrock의 Claude 3 Haiku 모델을 직접 호출하여 1위 이미지를 분석하고 답변을 생성합니다. 모든 코드 주석은 영어로 작성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup and Library Installation\n",
    "\n",
    "**[EN]** Install the necessary libraries, including boto3 for direct AWS communication.\n",
    "**[KR]** AWS와 직접 통신하기 위한 boto3를 포함하여, RAG 파이프라인에 필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"git+https://github.com/illuin-tech/colpali.git\"\n",
    "!pip install -q elasticsearch python-dotenv Pillow \"transformers>=4.41.0\" accelerate numpy torch ipywidgets boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Credentials and Configure Connections\n",
    "\n",
    "**[EN]** Load environment variables from `elastic.env` and `aws.env` to configure connections to Elastic Cloud and Amazon Bedrock.<br>\n",
    "**[KR]** `elastic.env`와 `aws.env` 파일에서 환경 변수를 로드하여 Elastic Cloud와 Amazon Bedrock 연결 정보를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch version: 8.11.0\n",
      "Connected to Bedrock in region: ap-northeast-2\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "import boto3\n",
    "\n",
    "# Load environment variables from .env files\n",
    "load_dotenv(dotenv_path='elastic.env')\n",
    "load_dotenv(dotenv_path='aws.env', override=True)\n",
    "\n",
    "# Elastic Cloud connection details\n",
    "ES_URL = os.getenv(\"ES_URL\")\n",
    "ES_API_KEY = os.getenv(\"ES_API_KEY\")\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(\"Please set ES_URL and ES_API_KEY in elastic.env\")\n",
    "\n",
    "# Amazon Bedrock connection details\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\", \"\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\", \"\")\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"ap-northeast-2\")\n",
    "if not AWS_ACCESS_KEY or not AWS_SECRET_KEY or AWS_ACCESS_KEY == \"<your-aws-access-key>\":\n",
    "    raise ValueError(\"Please set valid AWS credentials in aws.env\")\n",
    "\n",
    "# Create the Elasticsearch client\n",
    "if ':' in ES_URL and not ES_URL.startswith('http'):\n",
    "    es = Elasticsearch(cloud_id=ES_URL, api_key=ES_API_KEY, request_timeout=30)\n",
    "else:\n",
    "    es = Elasticsearch(hosts=[ES_URL], api_key=ES_API_KEY, request_timeout=30)\n",
    "print(f\"Connected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# Create the Bedrock client\n",
    "bedrock = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY,\n",
    "    region_name=AWS_REGION,\n",
    ")\n",
    "print(f\"Connected to Bedrock in region: {AWS_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Helpers and Load the Embedding Model\n",
    "\n",
    "**[EN]** Define all helper functions for visualization, vector generation, and the new Bedrock multimodal call. Load the `ColQwen` model to ensure vector dimensions match during search.<br>\n",
    "**[KR]** 시각화, 벡터 생성, 그리고 새로운 Bedrock 멀티모달 호출을 위한 모든 헬퍼 함수를 정의합니다. 검색 시 벡터 차원을 일치시키기 위해 `ColQwen` 모델을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229d03305f94471c9dab64e8d99c17b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f7e888e0c14e929c3d2b6ba732dd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'tsystems/colqwen2.5-3b-multilingual-v1.0' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "BEDROCK_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "def display_results(hits):\n",
    "    # This function renders search results as an HTML table.\n",
    "    if not hits:\n",
    "        print(\"No documents found.\")\n",
    "        return\n",
    "    html = \"<div style='display:flex; flex-wrap:wrap;'>\"\n",
    "    for i, hit in enumerate(hits):\n",
    "        doc_id = hit[\"_id\"]\n",
    "        score = hit[\"_score\"]\n",
    "        path = hit[\"_source\"].get(\"image_path\", \"\")\n",
    "        category = hit[\"_source\"].get(\"category\", \"N/A\")\n",
    "        try:\n",
    "            with open(path, \"rb\") as image_file:\n",
    "                img_str = base64.b64encode(image_file.read()).decode()\n",
    "                html += f\"<div style='margin:10px; padding:10px; border:1px solid #ddd; text-align:center; width: 220px;'><b>Rank #{i+1}</b><br><img src='data:image/png;base64,{img_str}' style='width:200px; height:auto; margin-top:5px;'><br><div style='font-size:12px; margin-top:5px;'><b>ID:</b> {doc_id[:15]}...<br><b>Score:</b> {score:.4f}<br><b>Category:</b> {category}</div></div>\"\n",
    "        except Exception:\n",
    "            html += f\"<div style='margin:10px; padding:10px; border:1px solid #ddd; text-align:center; width: 220px; height: 300px;'><b>Rank #{i+1}</b><br><div style='width:200px; height:200px; background-color:#f0f0f0; margin-top:5px; display:flex; align-items:center; justify-content:center; font-size:12px;'>Image not available</div><div style='font-size:12px; margin-top:5px;'><b>ID:</b> {doc_id[:15]}...<br><b>Score:</b> {score:.4f}<br><b>Category:</b> {category}</div></div>\"\n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def get_media_type(image_path):\n",
    "    # Determines the media type from the file extension.\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    return {\".jpeg\": \"image/jpeg\", \".jpg\": \"image/jpeg\", \".png\": \"image/png\", \".webp\": \"image/webp\"}.get(ext, \"image/jpeg\")\n",
    "\n",
    "def generate_llm_answer_with_image(bedrock_client, query_text, hits):\n",
    "    # Encodes the top image and generates an answer using Bedrock.\n",
    "    if not hits:\n",
    "        print(\"No documents found, cannot generate answer.\")\n",
    "        return\n",
    "    top_hit = hits[0]\n",
    "    image_path = top_hit[\"_source\"].get(\"image_path\")\n",
    "    if not image_path or not os.path.exists(image_path):\n",
    "        print(\"Top result has no valid image path.\")\n",
    "        return\n",
    "    print(f\"\\nAnalyzing top image for context: {image_path}\")\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    system_prompt = \"\"\"You are an intelligent document analysis assistant. Your task is to analyze a document image that a search system has retrieved in response to a user's query. Your response should be structured as follows:\\n\\n1.  **Relevance Assessment**: Start by explaining how relevant the document is to the user's query.\\n2.  **Summary**: Provide a concise summary of the document's key information.\\n3.  **Direct Answer**: Directly answer the user's query based on the document's content.\\n4.  **Contextual Explanation**: If the document is not a perfect match for the query, explain why it was likely the most relevant result found. You can describe the relationship between the user's query terms and the document's content. For instance, you could say, 'While this document does not specifically mention [a key term from the query], it discusses [a related topic in the document], making it the most relevant document found.'\"\"\"\n",
    "    \n",
    "    bedrock_request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "        \"max_tokens\": 2048,\n",
    "        \"system\": system_prompt,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": get_media_type(image_path), \"data\": image_base64}},\n",
    "                {\"type\": \"text\", \"text\": f\"User Query: '{query_text}'\\n\\nPlease analyze the provided document image based on the instructions in the system prompt.\"}\n",
    "            ]}]\n",
    "    }\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps(bedrock_request_body), modelId=BEDROCK_MODEL_ID,\n",
    "        contentType=\"application/json\", accept=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    result = response_body.get(\"content\", [{}])[0].get(\"text\", \"No response generated.\")\n",
    "    print(f\"\\n[FINAL LLM RESPONSE]:\\n{result}\")\n",
    "\n",
    "# Set up the device (GPU or CPU)\n",
    "device_map = \"cpu\"\n",
    "if torch.backends.mps.is_available(): device_map = \"mps\"\n",
    "elif torch.cuda.is_available(): device_map = \"cuda:0\"\n",
    "print(f\"Using device: {device_map}\")\n",
    "\n",
    "# Load the ColQwen model used for indexing.\n",
    "MODEL_NAME = \"tsystems/colqwen2.5-3b-multilingual-v1.0\"\n",
    "model = ColQwen2_5.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16 if device_map != \"cpu\" else torch.float32, device_map=device_map).eval()\n",
    "processor = ColQwen2_5_Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Embedding model '{MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "def create_colqwen_query_vectors(query_text, model, processor):\n",
    "    # Creates multi-vector embeddings for a text query.\n",
    "    inputs = processor.process_queries([query_text]).to(model.device)\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "\n",
    "def to_avg_vector(vectors):\n",
    "    # Calculates a single, normalized average vector.\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    norm = np.linalg.norm(avg_vector)\n",
    "    return (avg_vector / norm).tolist() if norm > 0 else avg_vector.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create and Display the Interactive RAG Chatbot\n",
    "\n",
    "**[EN]** This cell creates an interactive UI using `ipywidgets`. You can select a search strategy, input a query, and trigger the full multimodal RAG pipeline.<br>\n",
    "**[KR]** 이 셀은 `ipywidgets`를 사용하여 대화형 UI를 생성합니다. 검색 전략을 선택하고, 쿼리를 입력하여 전체 멀티모달 RAG 파이프라인을 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3186709/2523668283.py:77: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  query_input.on_submit(handle_text_submit)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed7c2c9b29e449fa65a98436c5732b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(RadioButtons(description='Search Mode:', layout=Layout(width='max-content'), options=('A. Colpa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. Define Constants and Main RAG Pipeline Function ---\n",
    "SEARCH_MODES = {\n",
    "    \"A. Colpali(colqwen) RAG search (Part 1)\": {\n",
    "        \"index\": \"colqwen-rvlcdip-demo-part1\",\n",
    "        \"multi_vector_field\": \"colqwen_vectors\"\n",
    "    },\n",
    "    \"B. Average RAG search (Part 2 - KNN Only)\": {\n",
    "        \"index\": \"colqwen-rvlcdip-demo-part2\",\n",
    "        \"avg_vector_field\": \"colqwen_avg_vector\"\n",
    "    },\n",
    "    \"C. Rescore RAG search (Part 2 - KNN + Rescore)\": {\n",
    "        \"index\": \"colqwen-rvlcdip-demo-part2\",\n",
    "        \"avg_vector_field\": \"colqwen_avg_vector\",\n",
    "        \"multi_vector_field\": \"colqwen_vectors\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_rag_pipeline(query_text, search_mode):\n",
    "    # This function contains the logic for all three search modes.\n",
    "    mode_config = SEARCH_MODES[search_mode]\n",
    "    index_name = mode_config[\"index\"]\n",
    "    es_query_body = None\n",
    "\n",
    "    print(f\"--- Running RAG for query: '{query_text}' (Mode: {search_mode}) ---\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        query_multi_vectors = create_colqwen_query_vectors(query_text, model, processor)\n",
    "        if not query_multi_vectors: raise ValueError(\"Failed to create query vectors.\")\n",
    "\n",
    "        if search_mode.startswith(\"A.\"):\n",
    "            es_query_body = {\"size\": 10, \"query\": {\"script_score\": {\"query\": {\"match_all\": {}}, \"script\": {\"source\": f\"maxSimDotProduct(params.query_vector, '{mode_config['multi_vector_field']}')\", \"params\": {\"query_vector\": query_multi_vectors}}}}, \"_source\": [\"image_path\", \"category\"]}\n",
    "        elif search_mode.startswith(\"B.\"):\n",
    "            query_avg_vector = to_avg_vector(query_multi_vectors)\n",
    "            es_query_body = {\"size\": 10, \"knn\": {\"field\": mode_config['avg_vector_field'], \"query_vector\": query_avg_vector, \"k\": 200, \"num_candidates\": 500}, \"_source\": [\"image_path\", \"category\"]}\n",
    "        else:\n",
    "            query_avg_vector = to_avg_vector(query_multi_vectors)\n",
    "            knn_query = {\"field\": mode_config['avg_vector_field'], \"query_vector\": query_avg_vector, \"k\": 200, \"num_candidates\": 500}\n",
    "            rescore_definition = {\"window_size\": 50, \"query\": {\"rescore_query\": {\"script_score\": {\"query\": {\"match_all\": {}}, \"script\": {\"source\": f\"maxSimDotProduct(params.query_vector, '{mode_config['multi_vector_field']}')\", \"params\": {\"query_vector\": query_multi_vectors}}}}, \"query_weight\": 0.0, \"rescore_query_weight\": 1.0}}\n",
    "            es_query_body = {\"size\": 10, \"knn\": knn_query, \"rescore\": rescore_definition, \"_source\": [\"image_path\", \"category\"]}\n",
    "\n",
    "        response = es.search(index=index_name, body=es_query_body)\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "        print(f\"\\n[VISUAL SEARCH RESULTS] - Retrieved {len(hits)} documents in {latency_ms:.2f} ms:\")\n",
    "        display_results(hits)\n",
    "        \n",
    "        # Generate answer from the top image using Bedrock\n",
    "        generate_llm_answer_with_image(bedrock, query_text, hits)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "# --- 2. Create and Display the UI Widgets ---\n",
    "query_input = widgets.Text(placeholder='Enter your query here and press Enter', description='Query:', layout=widgets.Layout(width='95%'))\n",
    "search_mode_selector = widgets.RadioButtons(options=list(SEARCH_MODES.keys()), description='Search Mode:', disabled=False, layout=widgets.Layout(width='max-content'))\n",
    "output_area = widgets.Output()\n",
    "example_queries = [\"Do you have a benefits policy change notice from HR?\", \"HR에서 보내온 복리후생 정책 변경 안내문이 있나?\"]\n",
    "example_buttons = [widgets.Button(description=q, layout=widgets.Layout(width='auto')) for q in example_queries]\n",
    "button_box = widgets.HBox([widgets.Label(\"Examples:\")] + example_buttons)\n",
    "\n",
    "def on_search_triggered(query):\n",
    "    if not query: \n",
    "        with output_area: output_area.clear_output(); print(\"Please enter a query.\")\n",
    "        return\n",
    "    with output_area: \n",
    "        output_area.clear_output(wait=True)\n",
    "        run_rag_pipeline(query, search_mode_selector.value)\n",
    "\n",
    "def handle_text_submit(sender): on_search_triggered(sender.value)\n",
    "def handle_button_click(button): query_input.value = button.description; on_search_triggered(button.description)\n",
    "\n",
    "query_input.on_submit(handle_text_submit)\n",
    "for btn in example_buttons: btn.on_click(handle_button_click)\n",
    "\n",
    "ui = widgets.VBox([search_mode_selector, button_box, query_input, output_area])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Clean Up Memory (Optional)\n",
    "\n",
    "**[EN]** As a best practice, explicitly delete the model and processor to free up GPU or system memory after the demonstration is complete.<br>\n",
    "**[KR]** 모범 사례로서, 데모가 완료된 후 모델과 프로세서를 명시적으로 삭제하여 GPU 또는 시스템 메모리를 확보합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor variables deleted.\n",
      "CUDA cache cleared.\n",
      "Memory cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del processor\n",
    "    print(\"Model and processor variables deleted.\")\n",
    "except NameError:\n",
    "    print(\"Model and processor variables not found, skipping deletion.\")\n",
    "\n",
    "if 'torch' in locals() and torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n",
    "elif 'torch' in locals() and torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared.\")\n",
    "\n",
    "# Call Python's garbage collector to clean up memory.\n",
    "gc.collect()\n",
    "print(\"Memory cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
