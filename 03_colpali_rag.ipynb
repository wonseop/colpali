{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Interactive Multimodal RAG\n",
    "\n",
    "**[EN]** This notebook implements a complete, interactive multimodal RAG demo. It uses Colpali for visual search and automatically detects available LLM backends (Ollama or AWS Bedrock) based on which `.env` files exist in your workspace.\n",
    "**[KR]** ì´ Notebookì€ ì™„ì „í•œ ëŒ€í™”í˜• ë©€í‹°ëª¨ë‹¬ RAG ë°ëª¨ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. Colpalië¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ , ì›Œí¬ìŠ¤í˜ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” `.env` íŒŒì¼ì— ë”°ë¼ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ë°±ì—”ë“œ(Ollama ë˜ëŠ” AWS Bedrock)ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup and Library Installation\n",
    "\n",
    "**[EN]** Install the necessary libraries, including boto3 for AWS and requests for Ollama.\n",
    "**[KR]** AWSìš© boto3ì™€ Ollamaìš© requestsë¥¼ í¬í•¨í•˜ì—¬, RAG íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"git+https://github.com/illuin-tech/colpali.git\"\n",
    "!pip install -q elasticsearch python-dotenv Pillow \"transformers>=4.41.0\" accelerate numpy torch ipywidgets requests boto3\n",
    "!pip install --upgrade jupyter jupyterlab ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load Credentials and Configure Connections\n",
    "\n",
    "**[EN]** Load environment variables from `elastic.env` and auto-detect LLM backends based on the existence of `ollama.env` and `aws.env` files.<br>\n",
    "**[KR]** `elastic.env`ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê³ , `ollama.env`ì™€ `aws.env` íŒŒì¼ì˜ ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ LLM ë°±ì—”ë“œë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from elasticsearch import Elasticsearch\n",
    "import requests\n",
    "\n",
    "# Load Elasticsearch environment variables\n",
    "load_dotenv(dotenv_path='elastic.env')\n",
    "\n",
    "ES_URL = os.getenv(\"ES_URL\")\n",
    "ES_API_KEY = os.getenv(\"ES_API_KEY\")\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(\"Please set ES_URL and ES_API_KEY in elastic.env\")\n",
    "\n",
    "# Create the Elasticsearch client\n",
    "if ':' in ES_URL and not ES_URL.startswith('http'):\n",
    "    es = Elasticsearch(cloud_id=ES_URL, api_key=ES_API_KEY, request_timeout=30, verify_certs=False, ssl_show_warn=False)\n",
    "else:\n",
    "    es = Elasticsearch(hosts=[ES_URL], api_key=ES_API_KEY, request_timeout=30, verify_certs=False, ssl_show_warn=False)\n",
    "print(f\"âœ… Connected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# --- Auto-detect available LLM backends ---\n",
    "AVAILABLE_LLM_BACKENDS = []\n",
    "\n",
    "# Check for OpenAI Compatible API (supports OpenAI, Ollama, and other compatible providers)\n",
    "OPENAI_BASE_URL = None\n",
    "OPENAI_API_KEY = None\n",
    "OPENAI_MODEL = None\n",
    "if os.path.exists('openai.env'):\n",
    "    load_dotenv(dotenv_path='openai.env', override=True)\n",
    "    OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n",
    "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")  # API key is optional\n",
    "    OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "    \n",
    "    try:\n",
    "        # Test connection by listing models (API key is optional)\n",
    "        headers = {\"Authorization\": f\"Bearer {OPENAI_API_KEY}\"} if OPENAI_API_KEY else {}\n",
    "        openai_test = requests.get(f\"{OPENAI_BASE_URL}/models\", headers=headers, timeout=10)\n",
    "        if openai_test.status_code == 200:\n",
    "            print(f\"âœ… OpenAI Compatible API detected at: {OPENAI_BASE_URL}\")\n",
    "            print(f\"   Using model: {OPENAI_MODEL}\")\n",
    "            AVAILABLE_LLM_BACKENDS.append(\"OpenAI\")\n",
    "        else:\n",
    "            # Some providers don't support /models endpoint, try anyway\n",
    "            print(f\"âœ… OpenAI Compatible API configured at: {OPENAI_BASE_URL}\")\n",
    "            print(f\"   Using model: {OPENAI_MODEL}\")\n",
    "            AVAILABLE_LLM_BACKENDS.append(\"OpenAI\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âš ï¸  openai.env exists but cannot connect to {OPENAI_BASE_URL}: {e}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  openai.env not found - OpenAI/Ollama backend disabled\")\n",
    "\n",
    "# Check for AWS Bedrock\n",
    "BEDROCK_CLIENT = None\n",
    "BEDROCK_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "if os.path.exists('aws.env'):\n",
    "    load_dotenv(dotenv_path='aws.env', override=True)\n",
    "    AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\", \"\")\n",
    "    AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\", \"\")\n",
    "    AWS_REGION = os.getenv(\"AWS_REGION\", \"ap-northeast-2\")\n",
    "    \n",
    "    if AWS_ACCESS_KEY and AWS_SECRET_KEY and AWS_ACCESS_KEY != \"<your-aws-access-key>\":\n",
    "        try:\n",
    "            import boto3\n",
    "            BEDROCK_CLIENT = boto3.client(\n",
    "                \"bedrock-runtime\",\n",
    "                aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                aws_secret_access_key=AWS_SECRET_KEY,\n",
    "                region_name=AWS_REGION,\n",
    "            )\n",
    "            print(f\"âœ… AWS Bedrock detected in region: {AWS_REGION}\")\n",
    "            AVAILABLE_LLM_BACKENDS.append(\"AWS Bedrock\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  aws.env exists but cannot connect to Bedrock: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  aws.env exists but credentials are not set properly\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  aws.env not found - AWS Bedrock backend disabled\")\n",
    "\n",
    "# Summary\n",
    "if not AVAILABLE_LLM_BACKENDS:\n",
    "    print(\"\\nâŒ No LLM backends available! Please create openai.env or aws.env\")\n",
    "else:\n",
    "    print(f\"\\nğŸ‰ Available LLM backends: {AVAILABLE_LLM_BACKENDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Helpers and Load the Embedding Model\n",
    "\n",
    "**[EN]** Define helper functions for visualization, vector generation, and LLM calls (supporting both Ollama and Bedrock). Load the `ColQwen` model for search.<br>\n",
    "**[KR]** ì‹œê°í™”, ë²¡í„° ìƒì„±, LLM í˜¸ì¶œ(Ollamaì™€ Bedrock ëª¨ë‘ ì§€ì›)ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ê²€ìƒ‰ì„ ìœ„í•´ `ColQwen` ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from colpali_engine.models import ColQwen3, ColQwen3Processor\n",
    "from transformers import AutoConfig\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an intelligent document analysis assistant. Your task is to analyze a document image that a search system has retrieved in response to a user's query. Your response should be structured as follows:\n",
    "\n",
    "1.  **Relevance Assessment**: Start by explaining how relevant the document is to the user's query.\n",
    "2.  **Summary**: Provide a concise summary of the document's key information.\n",
    "3.  **Direct Answer**: Directly answer the user's query based on the document's content.\n",
    "4.  **Contextual Explanation**: If the document is not a perfect match for the query, explain why it was likely the most relevant result found.\"\"\"\n",
    "\n",
    "def display_results(hits):\n",
    "    \"\"\"Renders search results as an HTML table.\"\"\"\n",
    "    if not hits:\n",
    "        print(\"No documents found.\")\n",
    "        return\n",
    "    html = \"<div style='display:flex; flex-wrap:wrap;'>\"\n",
    "    for i, hit in enumerate(hits):\n",
    "        doc_id = hit[\"_id\"]\n",
    "        score = hit[\"_score\"]\n",
    "        path = hit[\"_source\"].get(\"image_path\", \"\")\n",
    "        category = hit[\"_source\"].get(\"category\", \"N/A\")\n",
    "        try:\n",
    "            with open(path, \"rb\") as image_file:\n",
    "                img_str = base64.b64encode(image_file.read()).decode()\n",
    "                html += f\"<div style='margin:10px; padding:10px; border:1px solid #ddd; text-align:center; width: 220px;'><b>Rank #{i+1}</b><br><img src='data:image/png;base64,{img_str}' style='width:200px; height:auto; margin-top:5px;'><br><div style='font-size:12px; margin-top:5px;'><b>ID:</b> {doc_id[:15]}...<br><b>Score:</b> {score:.4f}<br><b>Category:</b> {category}</div></div>\"\n",
    "        except Exception:\n",
    "            html += f\"<div style='margin:10px; padding:10px; border:1px solid #ddd; text-align:center; width: 220px; height: 300px;'><b>Rank #{i+1}</b><br><div style='width:200px; height:200px; background-color:#f0f0f0; margin-top:5px; display:flex; align-items:center; justify-content:center; font-size:12px;'>Image not available</div><div style='font-size:12px; margin-top:5px;'><b>ID:</b> {doc_id[:15]}...<br><b>Score:</b> {score:.4f}<br><b>Category:</b> {category}</div></div>\"\n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "def get_media_type(image_path):\n",
    "    \"\"\"Determines the media type from the file extension.\"\"\"\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    return {\".jpeg\": \"image/jpeg\", \".jpg\": \"image/jpeg\", \".png\": \"image/png\", \".webp\": \"image/webp\"}.get(ext, \"image/jpeg\")\n",
    "\n",
    "def generate_with_openai(query_text, image_base64, image_path):\n",
    "    \"\"\"Generate answer using OpenAI Compatible API.\"\"\"\n",
    "    media_type = get_media_type(image_path)\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    openai_request = {\n",
    "        \"model\": OPENAI_MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:{media_type};base64,{image_base64}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": f\"User Query: '{query_text}'\\n\\nPlease analyze the provided document image.\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 2048\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{OPENAI_BASE_URL}/chat/completions\",\n",
    "        headers=headers,\n",
    "        json=openai_request,\n",
    "        timeout=120\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "def generate_with_bedrock(query_text, image_base64, image_path):\n",
    "    \"\"\"Generate answer using AWS Bedrock.\"\"\"\n",
    "    bedrock_request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": get_media_type(image_path), \"data\": image_base64}},\n",
    "                {\"type\": \"text\", \"text\": f\"User Query: '{query_text}'\\n\\nPlease analyze the provided document image.\"}\n",
    "            ]\n",
    "        }]\n",
    "    }\n",
    "    response = BEDROCK_CLIENT.invoke_model(\n",
    "        body=json.dumps(bedrock_request_body),\n",
    "        modelId=BEDROCK_MODEL_ID,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body.get(\"content\", [{}])[0].get(\"text\", \"No response generated.\")\n",
    "\n",
    "def generate_llm_answer_with_image(query_text, hits, llm_backend):\n",
    "    \"\"\"Encodes the top image and generates an answer using selected LLM backend.\"\"\"\n",
    "    if not hits:\n",
    "        print(\"No documents found, cannot generate answer.\")\n",
    "        return\n",
    "    top_hit = hits[0]\n",
    "    image_path = top_hit[\"_source\"].get(\"image_path\")\n",
    "    if not image_path or not os.path.exists(image_path):\n",
    "        print(\"Top result has no valid image path.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ” Analyzing top image with {llm_backend}: {image_path}\")\n",
    "    \n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "    try:\n",
    "        if llm_backend == \"OpenAI\":\n",
    "            result = generate_with_openai(query_text, image_base64, image_path)\n",
    "        elif llm_backend == \"AWS Bedrock\":\n",
    "            result = generate_with_bedrock(query_text, image_base64, image_path)\n",
    "        else:\n",
    "            result = \"Unknown LLM backend selected.\"\n",
    "        print(f\"\\n[FINAL LLM RESPONSE]:\\n{result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error calling {llm_backend}: {e}\")\n",
    "\n",
    "# Set up the device (GPU or CPU)\n",
    "device_map = \"cpu\"\n",
    "if torch.backends.mps.is_available(): device_map = \"mps\"\n",
    "elif torch.cuda.is_available(): device_map = \"cuda:0\"\n",
    "print(f\"Using device: {device_map}\")\n",
    "\n",
    "# Load the ColQwen model used for indexing.\n",
    "MODEL_NAME = \"TomoroAI/tomoro-colqwen3-embed-4b\"\n",
    "\n",
    "# Load config first and fix rope_scaling if None\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if hasattr(config, 'text_config') and config.text_config.rope_scaling is None:\n",
    "    config.text_config.rope_scaling = {\"mrope_section\": [24, 20, 20], \"type\": \"default\"}\n",
    "\n",
    "# Try flash_attention_2 first, fall back to sdpa if not available\n",
    "attn_impl = \"flash_attention_2\" if device_map != \"cpu\" else \"eager\"\n",
    "try:\n",
    "    import flash_attn\n",
    "except ImportError:\n",
    "    attn_impl = \"sdpa\" if device_map != \"cpu\" else \"eager\"\n",
    "    print(f\"flash-attn not installed, using '{attn_impl}' attention instead.\")\n",
    "\n",
    "model = ColQwen3.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16 if device_map != \"cpu\" else torch.float32,\n",
    "    device_map=device_map,\n",
    "    attn_implementation=attn_impl,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "processor = ColQwen3Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Embedding model '{MODEL_NAME}' loaded successfully.\")\n",
    "\n",
    "def create_colqwen_query_vectors(query_text, model, processor):\n",
    "    \"\"\"Creates multi-vector embeddings for a text query.\"\"\"\n",
    "    inputs = processor.process_queries([query_text]).to(model.device)\n",
    "    with torch.no_grad(): outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "\n",
    "def to_avg_vector(vectors):\n",
    "    \"\"\"Calculates a single, normalized average vector.\"\"\"\n",
    "    vectors_array = np.array(vectors)\n",
    "    avg_vector = np.mean(vectors_array, axis=0)\n",
    "    norm = np.linalg.norm(avg_vector)\n",
    "    return (avg_vector / norm).tolist() if norm > 0 else avg_vector.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create and Display the Interactive RAG Chatbot\n",
    "\n",
    "**[EN]** This cell creates an interactive UI using `ipywidgets`. You can select a search strategy, input a query, and trigger the full multimodal RAG pipeline.<br>\n",
    "**[KR]** ì´ ì…€ì€ `ipywidgets`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”í˜• UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì „ëµì„ ì„ íƒí•˜ê³ , ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì—¬ ì „ì²´ ë©€í‹°ëª¨ë‹¬ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# --- 1. Define Constants and Main RAG Pipeline Function ---\n",
    "SEARCH_MODES = {\n",
    "    \"A. Colpali(colqwen) RAG search (Part 1)\": {\n",
    "        \"index\": \"colqwen3-rvlcdip-demo-part1\",\n",
    "        \"multi_vector_field\": \"colqwen_vectors\"\n",
    "    },\n",
    "    \"B. Average RAG search (Part 2 - KNN Only)\": {\n",
    "        \"index\": \"colqwen3-rvlcdip-demo-part2\",\n",
    "        \"avg_vector_field\": \"colqwen_avg_vector\"\n",
    "    },\n",
    "    \"C. Rescore RAG search (Part 2 - KNN + Rescore)\": {\n",
    "        \"index\": \"colqwen3-rvlcdip-demo-part2\",\n",
    "        \"avg_vector_field\": \"colqwen_avg_vector\",\n",
    "        \"multi_vector_field\": \"colqwen_vectors\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def run_rag_pipeline(query_text, search_mode, llm_backend):\n",
    "    \"\"\"Main RAG pipeline function.\"\"\"\n",
    "    mode_config = SEARCH_MODES[search_mode]\n",
    "    index_name = mode_config[\"index\"]\n",
    "    es_query_body = None\n",
    "\n",
    "    print(f\"--- Running RAG for query: '{query_text}' ---\")\n",
    "    print(f\"    Search Mode: {search_mode}\")\n",
    "    print(f\"    LLM Backend: {llm_backend}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        query_multi_vectors = create_colqwen_query_vectors(query_text, model, processor)\n",
    "        if not query_multi_vectors: raise ValueError(\"Failed to create query vectors.\")\n",
    "\n",
    "        if search_mode.startswith(\"A.\"):\n",
    "            es_query_body = {\"size\": 10, \"query\": {\"script_score\": {\"query\": {\"match_all\": {}}, \"script\": {\"source\": f\"maxSimDotProduct(params.query_vector, '{mode_config['multi_vector_field']}')\", \"params\": {\"query_vector\": query_multi_vectors}}}}, \"_source\": [\"image_path\", \"category\"]}\n",
    "        elif search_mode.startswith(\"B.\"):\n",
    "            query_avg_vector = to_avg_vector(query_multi_vectors)\n",
    "            es_query_body = {\"size\": 10, \"knn\": {\"field\": mode_config['avg_vector_field'], \"query_vector\": query_avg_vector, \"k\": 200, \"num_candidates\": 500}, \"_source\": [\"image_path\", \"category\"]}\n",
    "        else:\n",
    "            query_avg_vector = to_avg_vector(query_multi_vectors)\n",
    "            knn_query = {\"field\": mode_config['avg_vector_field'], \"query_vector\": query_avg_vector, \"k\": 200, \"num_candidates\": 500}\n",
    "            rescore_definition = {\"window_size\": 50, \"query\": {\"rescore_query\": {\"script_score\": {\"query\": {\"match_all\": {}}, \"script\": {\"source\": f\"maxSimDotProduct(params.query_vector, '{mode_config['multi_vector_field']}')\", \"params\": {\"query_vector\": query_multi_vectors}}}}, \"query_weight\": 0.0, \"rescore_query_weight\": 1.0}}\n",
    "            es_query_body = {\"size\": 10, \"knn\": knn_query, \"rescore\": rescore_definition, \"_source\": [\"image_path\", \"category\"]}\n",
    "\n",
    "        response = es.search(index=index_name, body=es_query_body)\n",
    "        end_time = time.time()\n",
    "        latency_ms = (end_time - start_time) * 1000\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "        print(f\"\\n[VISUAL SEARCH RESULTS] - Retrieved {len(hits)} documents in {latency_ms:.2f} ms:\")\n",
    "        display_results(hits)\n",
    "        \n",
    "        # Generate answer from the top image using selected LLM backend\n",
    "        generate_llm_answer_with_image(query_text, hits, llm_backend)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "\n",
    "# --- 2. Create and Display the UI Widgets ---\n",
    "query_input = widgets.Text(placeholder='Enter your query here and press Enter', description='Query:', layout=widgets.Layout(width='95%'))\n",
    "search_mode_selector = widgets.RadioButtons(options=list(SEARCH_MODES.keys()), description='Search Mode:', disabled=False, layout=widgets.Layout(width='max-content'))\n",
    "\n",
    "# LLM Backend selector - only show available backends\n",
    "if AVAILABLE_LLM_BACKENDS:\n",
    "    llm_backend_selector = widgets.RadioButtons(\n",
    "        options=AVAILABLE_LLM_BACKENDS,\n",
    "        value=AVAILABLE_LLM_BACKENDS[0],\n",
    "        description='LLM Backend:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='max-content')\n",
    "    )\n",
    "else:\n",
    "    llm_backend_selector = widgets.HTML(value=\"<b style='color:red;'>No LLM backend available. Please create ollama.env or aws.env</b>\")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "example_queries = [\"Do you have a benefits policy change notice from HR?\", \"HRì—ì„œ ë³´ë‚´ì˜¨ ë³µë¦¬í›„ìƒ ì •ì±… ë³€ê²½ ì•ˆë‚´ë¬¸ì´ ìˆë‚˜?\"]\n",
    "example_buttons = [widgets.Button(description=q, layout=widgets.Layout(width='auto')) for q in example_queries]\n",
    "button_box = widgets.HBox([widgets.Label(\"Examples:\")] + example_buttons)\n",
    "\n",
    "def on_search_triggered(query):\n",
    "    if not query: \n",
    "        with output_area: output_area.clear_output(); print(\"Please enter a query.\")\n",
    "        return\n",
    "    if not AVAILABLE_LLM_BACKENDS:\n",
    "        with output_area: output_area.clear_output(); print(\"âŒ No LLM backend available!\")\n",
    "        return\n",
    "    with output_area: \n",
    "        output_area.clear_output(wait=True)\n",
    "        run_rag_pipeline(query, search_mode_selector.value, llm_backend_selector.value)\n",
    "\n",
    "def handle_text_submit(sender): on_search_triggered(sender.value)\n",
    "def handle_button_click(button): query_input.value = button.description; on_search_triggered(button.description)\n",
    "\n",
    "query_input.on_submit(handle_text_submit)\n",
    "for btn in example_buttons: btn.on_click(handle_button_click)\n",
    "\n",
    "# Create UI layout with LLM backend selector\n",
    "selector_box = widgets.HBox([search_mode_selector, widgets.HTML(value=\"&nbsp;&nbsp;&nbsp;\"), llm_backend_selector])\n",
    "ui = widgets.VBox([selector_box, button_box, query_input, output_area])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Clean Up Memory (Optional)\n",
    "\n",
    "**[EN]** As a best practice, explicitly delete the model and processor to free up GPU or system memory after the demonstration is complete.<br>\n",
    "**[KR]** ëª¨ë²” ì‚¬ë¡€ë¡œì„œ, ë°ëª¨ê°€ ì™„ë£Œëœ í›„ ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œí•˜ì—¬ GPU ë˜ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ë¥¼ í™•ë³´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# try:\n",
    "#     del model\n",
    "#     del processor\n",
    "#     print(\"Model and processor variables deleted.\")\n",
    "# except NameError:\n",
    "#     print(\"Model and processor variables not found, skipping deletion.\")\n",
    "\n",
    "# if 'torch' in locals() and torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(\"CUDA cache cleared.\")\n",
    "# elif 'torch' in locals() and torch.backends.mps.is_available():\n",
    "#     torch.mps.empty_cache()\n",
    "#     print(\"MPS cache cleared.\")\n",
    "\n",
    "# # Call Python's garbage collector to clean up memory.\n",
    "# gc.collect()\n",
    "# print(\"Memory cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
