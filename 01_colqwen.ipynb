{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Visual Document Search with ColQwen and Elasticsearch `rank_vectors`\n",
    "\n",
    "This notebook demonstrates how to reproduce and enhance the concepts from the Elastic Search Labs blog post '[Searching complex documents with ColPali](https://www.elastic.co/search-labs/blog/elastiacsearch-colpali-document-search)'.\n",
    "\n",
    "**Key Enhancements:**\n",
    "\n",
    "1.  **Model Upgrade**: We use the **`tsystems/colqwen2.5-3b-multilingual-v1.0`** model, a top-performing model on the ViDoRe benchmark.\n",
    "2.  **Real-world Dataset**: We use sample images from the **RVL-CDIP** dataset to demonstrate capabilities on complex, real-world documents.\n",
    "3.  **Robust Connection Logic**: The connection logic automatically detects whether the environment variable specifies a **Cloud ID** or a **Host URL**, ensuring a stable connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup Environment\n",
    "\n",
    "**[EN]** Install the necessary libraries.<br>\n",
    "**[KR]** í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"git+https://github.com/illuin-tech/colpali.git\"\n",
    "!pip install -q \"transformers>=4.41.0\" accelerate Pillow elasticsearch python-dotenv tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Set Environment Variables\n",
    "\n",
    "**[EN]** Import necessary libraries and load environment variables from the `elastic.env` file.<br>\n",
    "**[KR]** í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•˜ê³ , `elastic.env` íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "dotenv_path = 'elastic.env'\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "ES_URL = os.getenv(\"ES_URL\")\n",
    "ES_API_KEY = os.getenv(\"ES_API_KEY\")\n",
    "\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(f\"Please ensure '{dotenv_path}' contains ES_URL and ES_API_KEY.\")\n",
    "\n",
    "INDEX_NAME = \"colqwen-rvlcdip-demo-part1\"\n",
    "VECTOR_FIELD_NAME = \"colqwen_vectors\"\n",
    "SAMPLED_DATA_DIR = \"samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the ColQwen Model\n",
    "\n",
    "**[EN]** Load the `tsystems/colqwen2.5-3b-multilingual-v1.0` model and its processor.<br>\n",
    "**[KR]** `tsystems/colqwen2.5-3b-multilingual-v1.0` ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "device_map = \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    device_map = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_map = \"cuda:0\"\n",
    "print(f\"Using device: {device_map}\")\n",
    "\n",
    "MODEL_NAME = \"tsystems/colqwen2.5-3b-multilingual-v1.0\"\n",
    "model = ColQwen2_5.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16 if device_map != \"cpu\" else torch.float32,\n",
    "    device_map=device_map\n",
    ").eval()\n",
    "processor = ColQwen2_5_Processor.from_pretrained(MODEL_NAME)\n",
    "print(f\"Model '{MODEL_NAME}' loaded successfully on device '{model.device}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load RVL-CDIP Sample Data\n",
    "\n",
    "**[EN]** Load the list of RVL-CDIP sample image files from the `samples` directory.<br>\n",
    "**[KR]** `samples` ë””ë ‰í„°ë¦¬ì—ì„œ RVL-CDIP ìƒ˜í”Œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SAMPLED_DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Sample data directory not found at '{SAMPLED_DATA_DIR}'. Please create it in the same directory as this notebook.\")\n",
    "\n",
    "image_paths = []\n",
    "for category_dir in os.listdir(SAMPLED_DATA_DIR):\n",
    "    full_category_path = os.path.join(SAMPLED_DATA_DIR, category_dir)\n",
    "    if os.path.isdir(full_category_path):\n",
    "        image_paths.extend(glob.glob(os.path.join(full_category_path, '*.*')))\n",
    "print(f\"Found {len(image_paths)} sample images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Embedding Helper Functions\n",
    "\n",
    "**[EN]** Define helper functions to generate multi-vector embeddings for document images and text queries.<br>\n",
    "**[KR]** ë¬¸ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì§ˆì˜ì˜ ë‹¤ì¤‘ ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colqwen_document_vectors(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor.process_images([image]).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.cpu().to(torch.float32).numpy().tolist()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_colqwen_query_vectors(query_text):\n",
    "    inputs = processor.process_queries([query_text]).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.cpu().to(torch.float32).numpy().tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 & Step 6 : Connect to Elasticsearch and Create an Index and Index the Data\n",
    "\n",
    "**[EN]** Connect to Elasticsearch and create an index with a `rank_vectors` field and index the loaded sample images into Elasticsearch. <br>\n",
    "**[KR]** Elasticsearchì— ì—°ê²°í•˜ê³  `rank_vectors` í•„ë“œë¥¼ í¬í•¨í•˜ëŠ” ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•œ í›„ ë¡œë“œëœ ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì„ Elasticsearchì— ì¸ë±ì‹±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This combined cell handles both index creation and document indexing\n",
    "# to ensure data integrity based on the document count.\n",
    "\n",
    "# --- Logic from original Step 5: Connect to Elasticsearch and Manage Index ---\n",
    "\n",
    "# Automatically detect if ES_URL is a Cloud ID or a Host URL.\n",
    "if ':' in ES_URL and not ES_URL.startswith('http'):\n",
    "    # Connect using Cloud ID\n",
    "    print(\"Connecting using Cloud ID...\")\n",
    "    es = Elasticsearch(\n",
    "        cloud_id=ES_URL,\n",
    "        api_key=ES_API_KEY,\n",
    "        request_timeout=20\n",
    "    )\n",
    "else:\n",
    "    # Connect using Host URL\n",
    "    print(\"Connecting using Host URL...\")\n",
    "    es = Elasticsearch(\n",
    "        hosts=[ES_URL],\n",
    "        api_key=ES_API_KEY,\n",
    "        request_timeout=20,\n",
    "        verify_certs=False,  # Disable SSL certificate verification for self-signed certs\n",
    "        ssl_show_warn=False  # Suppress SSL warnings\n",
    "    )\n",
    "\n",
    "print(f\"Connected to Elasticsearch version: {es.info()['version']['number']}\")\n",
    "\n",
    "# Define the index mapping\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        VECTOR_FIELD_NAME: {\"type\": \"rank_vectors\", \"dims\": 128},\n",
    "        \"image_path\": {\"type\": \"keyword\"},\n",
    "        \"category\": {\"type\": \"keyword\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Logic from original Step 6: Validate and Conditionally Index Documents ---\n",
    "\n",
    "# Default to an invalid state to trigger re-indexing if checks fail.\n",
    "is_index_valid = False\n",
    "\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    try:\n",
    "        doc_count = es.count(index=INDEX_NAME)['count']\n",
    "        print(f\"Index '{INDEX_NAME}' already exists with {doc_count} documents.\")\n",
    "        if doc_count == 1600:\n",
    "            is_index_valid = True\n",
    "    except Exception as e:\n",
    "        print(f\"Could not retrieve document count for existing index. Will recreate. Error: {e}\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' not found.\")\n",
    "\n",
    "# If the index is not valid (doesn't exist or has the wrong doc count),\n",
    "# delete it, recreate it, and re-index all documents.\n",
    "if not is_index_valid:\n",
    "    print(f\"Index state is not valid. Starting complete re-indexing process...\")\n",
    "    \n",
    "    # 1. Delete the index if it exists, to ensure a clean state.\n",
    "    if es.indices.exists(index=INDEX_NAME):\n",
    "        print(f\"Deleting existing index '{INDEX_NAME}'...\")\n",
    "        es.indices.delete(index=INDEX_NAME)\n",
    "    \n",
    "    # 2. Create a new, empty index with the correct mapping.\n",
    "    print(f\"Creating new index '{INDEX_NAME}'...\")\n",
    "    es.indices.create(index=INDEX_NAME, mappings=mapping)\n",
    "    print(f\"Index created successfully.\")\n",
    "\n",
    "    # 3. Index all 1600 documents.\n",
    "    MAX_DOCS_TO_INDEX = 1600\n",
    "    docs_to_index = image_paths[:min(len(image_paths), MAX_DOCS_TO_INDEX)]\n",
    "    print(f\"Indexing {len(docs_to_index)} documents...\")\n",
    "\n",
    "    for path in tqdm(docs_to_index, desc=\"Indexing Documents\"):\n",
    "        doc_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        category = os.path.basename(os.path.dirname(path))\n",
    "        vectors = create_colqwen_document_vectors(path)\n",
    "        \n",
    "        if vectors:\n",
    "            es_doc = {\n",
    "                VECTOR_FIELD_NAME: vectors,\n",
    "                \"image_path\": path,\n",
    "                \"category\": category\n",
    "            }\n",
    "            es.index(index=INDEX_NAME, id=doc_id, document=es_doc)\n",
    "\n",
    "    es.indices.refresh(index=INDEX_NAME)\n",
    "    print(\"\\nIndexing complete.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' is valid with 1600 documents. Skipping all steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Search and Visualize Results\n",
    "\n",
    "**[EN]** Perform a search using `script_score` and new `maxSimDotProduct` function to calculate the similarity between our query and the image vectors in Elasticsearch and visualize the results.<br>\n",
    "**[KR]** `script_score`ì™€ ìƒˆë¡œ ì¶œì‹œëœ `maxSimDotProduct`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ì™€ ì´ë¯¸ì§€ ë²¡í„° ê°„ì˜ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ì—¬ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³   ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary libraries\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "def remove_query_vector_from_explanation(explanation_obj):\n",
    "    \"\"\"\n",
    "    Recursively traverses the explanation object to remove the lengthy 'query_vector' parameter\n",
    "    from both structured 'params' and string-based 'description' fields.\n",
    "    \"\"\"\n",
    "    if isinstance(explanation_obj, dict):\n",
    "        if 'description' in explanation_obj and isinstance(explanation_obj['description'], str):\n",
    "            explanation_obj['description'] = re.sub(\n",
    "                r\"query_vector=\\[\\[.*?\\]\\]\", \n",
    "                \"query_vector=[...vector omitted for brevity...]\", \n",
    "                explanation_obj['description']\n",
    "            )\n",
    "        if 'params' in explanation_obj and 'query_vector' in explanation_obj['params']:\n",
    "            explanation_obj['params']['query_vector'] = \"[...vector omitted for brevity...]\"\n",
    "        for key, value in explanation_obj.items():\n",
    "            remove_query_vector_from_explanation(value)\n",
    "    elif isinstance(explanation_obj, list):\n",
    "        for item in explanation_obj:\n",
    "            remove_query_vector_from_explanation(item)\n",
    "    return explanation_obj\n",
    "\n",
    "def search_and_display_with_explain(query, es_client):\n",
    "    print(f\"\\nSearching for: '{query}'\")\n",
    "    query_vectors = create_colqwen_query_vectors(query)\n",
    "    \n",
    "    # This is the original, full \"late interaction\" search query.\n",
    "    es_query = {\n",
    "        \"_source\": [\"image_path\", \"category\"],\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": f\"maxSimDotProduct(params.query_vector, '{VECTOR_FIELD_NAME}')\",\n",
    "                    \"params\": {\"query_vector\": query_vectors},\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"size\": 5,\n",
    "        \"explain\": True  # Enable the Explain API\n",
    "    }\n",
    "    \n",
    "    # --- Measure Search Latency ---\n",
    "    start_time = time.time()\n",
    "    results = es_client.search(index=INDEX_NAME, body=es_query)\n",
    "    end_time = time.time()\n",
    "    latency_ms = (end_time - start_time) * 1000\n",
    "    print(f\"ğŸš€ Search Latency: {latency_ms:.2f} ms\")\n",
    "    # --- End of Measurement ---\n",
    "\n",
    "    hits = results[\"hits\"][\"hits\"]\n",
    "    \n",
    "    # --- Display visual results ---\n",
    "    html = \"<table><tr>\"\n",
    "    for hit in hits:\n",
    "        doc_id = hit[\"_id\"]\n",
    "        score = hit[\"_score\"]\n",
    "        path = hit[\"_source\"][\"image_path\"]\n",
    "        category = hit[\"_source\"][\"category\"]\n",
    "        try:\n",
    "            with open(path, \"rb\") as image_file:\n",
    "                img_str = base64.b64encode(image_file.read()).decode()\n",
    "                html += f\"\"\"\n",
    "                <td style='text-align: center; vertical-align: top; padding: 10px; border: 1px solid #ddd;'>\n",
    "                    <img src='data:image/png;base64,{img_str}' width='200'><br>\n",
    "                    <b>ID:</b> {doc_id}<br>\n",
    "                    <b>Score:</b> {score:.4f}<br>\n",
    "                    <b>Category:</b> {category}\n",
    "                </td>\n",
    "                \"\"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display image {path}: {e}\")\n",
    "    html += \"</tr></table>\"\n",
    "    \n",
    "    if hits:\n",
    "        display(HTML(html))\n",
    "        \n",
    "        # --- Display the cleaned explanation for the top result ---\n",
    "        top_hit_explanation = hits[0].get(\"_explanation\")\n",
    "        if top_hit_explanation:\n",
    "            print(\"\\n--- Explanation for Top Result (Rank #1) ---\")\n",
    "            cleaned_explanation = remove_query_vector_from_explanation(top_hit_explanation)\n",
    "            print(json.dumps(cleaned_explanation, indent=2))\n",
    "            \n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "# Perform a series of searches to demonstrate the model's capabilities with explanations.\n",
    "search_and_display_with_explain(\"Do you have a benefits policy change notice from HR?\", es)\n",
    "search_and_display_with_explain(\"ì¸ì‚¬íŒ€ì—ì„œ ë³´ë‚´ì˜¨ ë³µë¦¬í›„ìƒ ì •ì±… ë³€ê²½ ì•ˆë‚´ë¬¸ì´ ìˆë‚˜?\", es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Free up Memory\n",
    "\n",
    "**[EN]** As a best practice, explicitly delete the model and processor to free up GPU or system memory after the demonstration is complete.<br>\n",
    "**[KR]** ëª¨ë¸ì´ ì‚¬ìš©í•œ ë©”ëª¨ë¦¬ë¥¼ í•´ì œí•˜ê¸° ìœ„í•´ ì»¤ë„ì„ ê°•ì œë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del processor\n",
    "\n",
    "import gc\n",
    "\n",
    "if 'torch' in locals() and torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n",
    "elif 'torch' in locals() and torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"MPS cache cleared.\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"Memory cleanup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
